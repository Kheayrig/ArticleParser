<h1>Парсер статей:</h1>

Достаёт основную текстовую информацию с сайта, форматирует её и сохраняет в файле

<h3>Установка</h3>

1) Чтобы добавить кастомные настройки, добавьте в папку ".\articleParser\app\configs" файл .env с переменными:<br>
<code>CUR_DIR='path/to/dir'<br>
TXT_SETTINGS_PATH='path/to/file/with/txt/settings.json'<br>
DOCX_SETTINGS_PATH='path/to/file/with/docx/settings.json'<br></code>
Это необязательно, если программа не найдёт файл, то будет работать с дефолтными настройками. <br>
Образцы файлов настроек лежат по этому же адресу<br>

2) Установите зависимости: pip install -r requirements.txt

3) Перейдите в папку ".\articleParser\"

4) Введите: <code>python main.py url</code>, подставив ссылку вместо url

5) Возможный вариант команды: <br>
<code>python main.py https://lenta.ru/news/2023/03/30/new-career/ </code>
<br><code>python main.py https://lenta.ru/news/2023/03/30/new-career/ --ext docx</code>, 
если хотите сохранить в формате docx (по умолчанию - txt)

<h3>Алгоритм работы:</h3>

1) Вводим ссылку(url) нужной страницы

2) PageGetter: Получает html страницы по url

3) ArticleFormatter: удаляет лишние части страницы 
    (header, footer, head, реклама, навигация, скрипты, стили) - там тоже есть текст, но он нам не нужен. 
    Из оставшегося дерева выбирает только теги, содержащие текст. 
    Проверяет, чтобы они не оказались пустыми строками или переходом строки. 
    (Очень много такого мусора было на сайте gazeta.ru) 
    Возвращает список тегов-"абзацев"

4) Выбираем форматтер(txt или docx)

5) ToTxtFormatter: возвращает отформатированный txt файл, принимает на вход FormatSettings с настройками

6) ToDocxFormatter: возвращает отформатированный docx файл, принимает на вход DocxSettings(чуть больше настроек)

7) Сохраняет либо в заданную папку, либо в ".\articleParser\tests". Название файла задаётся автоматически по url, как и расположение.
!Важно! Меняет "?" на "@" в названии файла, тк ОС не поддерживает этот символ в имени файла.


<h3>Тестирование:</h3>

Список сайтов:

1) https://www.gazeta.ru/social/news/2023/03/30/20094211.shtml?updated

2) https://lenta.ru/news/2023/03/30/new-career/

3) https://ria.ru/20230402/neft-1862517152.html

4) https://habr.com/ru/post/280238/

5) https://vk.com/tensor_company?w=wall-33050067_1854


<h3>Проблемы, которые всё ещё остались:</h3>

1) Одной из главных задач было убрать рекламу, а она часто хранится просто в "a" тегах. На данный момент он считает 
хорошими ссылками те, что находятся внутри обычного текста(если рядом есть хоть один сестринский непустой элемент NavigableString)
Поэтому иногда может затирать нужные ссылки, но это редко. 

2) Мусор в скрытых элементах. BeautifulSoup не позволяет заглянуть в css элемента. Можно попробовать проверить через selenium. 
(Как последней проверкой перед добавлением: посмотреть классы/id, найти через них элементы в selenium, 
может быть заглянуть в родителей до n уровня, проверить display).


3) Оптимизация - много сравнений строк и поисков по строкам. 