<h1>Парсер статей:</h1>

Достаёт основную текстовую информацию с сайта, форматирует её и сохраняет в файле

<h3>Установка</h3>

1) Чтобы добавить кастомные настройки, добавьте в папку ".\articleParser\app\configs" файл .env с переменными:<br>
CUR_DIR='path/to/dir'<br>
TXT_SETTINGS_PATH='path/to/file/with/txt/settings.json'<br>
DOCX_SETTINGS_PATH='path/to/file/with/docx/settings.json'<br>
Это необязательно, если программа не найдёт файл, то будет работать с дефолтными настройками. <br>
Образцы файлов настроек лежат по этому же адресу<br>

2) Установите зависимости: pip install -r requirements.txt

3) Запустите main.py в ".\articleParser\app\"

<h3>Алгоритм работы:</h3>

1) Вводим ссылку(url) нужной страницы

2) PageGetter: Получает html страницы по url

3) ArticleFormatter: удаляет лишние части страницы<br>
    (header, footer, head, реклама, навигация, скрипты, стили) - там тоже есть текст, но он нам не нужен.<br>
    Из оставшегося дерева выбирает только теги, содержащие текст. <br>
    Проверяет, чтобы они не оказались пустыми строками или переходом строки.<br>
    (Очень много такого мусора было на сайте gazeta.ru)<br>
    Возвращает список тегов-"абзацев"

4) Выбираем форматтер(txt или docx)

5) ToTxtFormatter: возвращает отформатированный txt файл, принимает на вход FormatSettings с настройками

6) ToDocxFormatter: возвращает отформатированный docx файл, принимает на вход DocxSettings(чуть больше настроек)

7) Сохраняет либо в заданную папку, либо в ".\articleParser\tests". Название файла задаётся автоматически по url, как и расположение.
!Важно! Меняет "?" на "@" в названии файла, тк ОС не поддерживает этот символ в имени файла.


<h3>Тестирование:</h3>

Список сайтов:

1) https://www.gazeta.ru/social/news/2023/03/30/20094211.shtml?updated

2) https://lenta.ru/news/2023/03/30/new-career/

3) https://ria.ru/20230402/neft-1862517152.html

4) https://habr.com/ru/post/280238/

5) https://vk.com/tensor_company?w=wall-33050067_1854


<h3>Проблемы, которые всё ещё остались:</h3>

1) Одной из главных задач было убрать рекламу, а она часто хранится просто в "a" тегах. На данный момент он считает<br>
хорошими ссылками те, что находятся внутри обычного текста(если рядом есть хоть один сестринский непустой элемент NavigableString)<br>
Поэтому иногда может затирать нужные ссылки, но это редко.<br>

2) Мусор в скрытых элементах. BeautifulSoup не позволяет заглянуть в css элемента. Можно попробовать проверить через selenium.<br>
(Как последней проверкой перед добавлением: посмотреть классы/id, найти через них элементы в selenium,<br>
может быть заглянуть в родителей до n уровня, проверить display).


3) Оптимизация - много сравнений строк и поисков по строкам. 